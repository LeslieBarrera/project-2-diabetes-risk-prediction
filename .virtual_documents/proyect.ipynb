


# Import necessary libraries
import os
from kaggle.api.kaggle_api_extended import KaggleApi
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import numpy as np

# Step 1: Set up Kaggle API credentials
# Ensure the Kaggle API key (kaggle.json) is in the specified directory
os.environ['KAGGLE_CONFIG_DIR'] = '/path/to/your/project/'  # Replace with your actual path to kaggle.json

# Step 2: Authenticate with Kaggle API and download the dataset
api = KaggleApi()
api.authenticate()

# Dataset details
dataset_name = "alexteboul/diabetes-health-indicators-dataset"
destination = "./data"  # Directory to store the downloaded dataset

# Create the destination directory if it does not exist
os.makedirs(destination, exist_ok=True)

# Download and unzip the dataset
print(f"Downloading dataset: {dataset_name}...")
api.dataset_download_files(dataset_name, path=destination, unzip=True)
print(f"Dataset downloaded and extracted to: {destination}")

# Step 3: Locate the downloaded CSV file
dataset_file = next(
    (os.path.join(destination, file_name) for file_name in os.listdir(destination) if file_name.endswith(".csv")),
    None
)

# Step 4: Verify the CSV file and load it
if dataset_file is None:
    raise FileNotFoundError("No CSV file found in the downloaded dataset.")
else:
    print(f"CSV file found: {dataset_file}")

# Load the dataset into a pandas DataFrame
df = pd.read_csv(dataset_file)
print(f"Dataset loaded successfully. Shape: {df.shape}")

# Display the first few rows of the dataset
print(df.head())





# Import necessary libraries
import pandas as pd

# Load the dataset into a DataFrame
df = pd.read_csv(dataset_file)

# 1. Explore the dataset

print(f"\nShape of the dataset: {df.shape}")  # Dataset shape

print("\nDataset info:")
print(df.info())  # Data types and non-null counts

print("\nMissing values per column:")
print(df.isnull().sum())  # Missing values count

duplicates_count = df.duplicated().sum()
print(f"\nNumber of duplicate rows: {duplicates_count}")  # Duplicate rows count

print("\nDescriptive statistics for numerical columns:")
print(df.describe())  # Summary stats for numerical columns

if 'Diabetes_012' in df.columns:
    print("\nValue counts for the target column 'Diabetes_012':")
    print(df['Diabetes_012'].value_counts())  # Target column distribution

# 2. Clean the dataset
if duplicates_count > 0:
    df = df.drop_duplicates()
    print(f"\nRemoved {duplicates_count} duplicate rows. New shape: {df.shape}")

missing_values_count = df.isnull().sum().sum()
if missing_values_count > 0:
    df = df.dropna()
    print(f"\nRemoved rows with missing values. New shape: {df.shape}")

df.columns = df.columns.str.lower().str.replace(' ', '_')
print("\nStandardized column names:")
print(df.columns)





df.describe()



# Remove outliers only from the columns 'bmi' and 'age' as they are continuous features
columns_to_clean = ['bmi', 'age']

for col in columns_to_clean:
    q1 = df[col].quantile(0.25)  # First quartile
    q3 = df[col].quantile(0.75)  # Third quartile
    iqr = q3 - q1  # Interquartile range

    # Lower and upper bounds
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    # Filter the dataset to keep only values within the bounds
    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]

print(f"Dataset after removing outliers in 'bmi' and 'age': {df.shape}")



# 3. reviw if the  y  rariable is valance 
# 4. Split into training and testing

from collections import Counter
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

# Check the class distribution
print("Class distribution before balancing:")
class_counts = Counter(df['diabetes_012'])
print(class_counts)

# Separate features (X) and target variable (y)
X = df.drop(columns=['diabetes_012'])
y = df['diabetes_012']

# Apply SMOTE to balance the classes
smote = SMOTE(random_state=42)
X_balanced, y_balanced = smote.fit_resample(X, y)

# Check the new class distribution
print("Class distribution after applying SMOTE:")
balanced_class_counts = Counter(y_balanced)
print(balanced_class_counts)

# Split into training and testing (after balancing)
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)

print(f"Training data: {X_train.shape}, Testing data: {X_test.shape}")



# 5. Correlation analysis between features (to check for highly similar columns and assess the need for removal)

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Ensure X_train (training data) is defined after the split
# 1. Calculate the correlation matrix
correlation_matrix = X_train.corr()

# 2. Visualize the correlation matrix with numbers inside each square
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', square=True, cbar=True)
plt.title("Correlation Heatmap between Features (with values)")
plt.show()

# 3. Identify pairs of highly correlated features
threshold = 0.9  # Define a threshold for high correlation
high_corr_pairs = []

# Iterate over the correlation matrix to find high correlations
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > threshold:
            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j],
                                    correlation_matrix.iloc[i, j]))

# Display highly correlated feature pairs
print("Pairs of features with high correlation (>|0.9|):")
for pair in high_corr_pairs:
    print(f"{pair[0]} and {pair[1]} - Correlation: {pair[2]:.2f}")

# 4. Optional: Drop one variable from each pair of highly correlated features
# For simplicity, drop the second feature in each pair:
features_to_drop = set([pair[1] for pair in high_corr_pairs])
X_train_reduced = X_train.drop(columns=features_to_drop)
X_test_reduced = X_test.drop(columns=features_to_drop)

print(f"Features removed due to high correlation: {features_to_drop}")
print(f"Dataset dimensions after feature reduction: {X_train_reduced.shape}")



# # Mostrar todas las columnas del DataFrame
# print("Columnas disponibles en el DataFrame:")
# print(df.columns)





from sklearn.preprocessing import StandardScaler  

# Lista de características continuas que se escalarán
continuous_features = ['age', 'bmi', 'income', 'menthlth', 'physhlth', 'education']  # Ajustar si es necesario

# Escalador StandardScaler (Media=0, Desviación Estándar=1)
scaler = StandardScaler()

# Escalar solo las características continuas en el conjunto de entrenamiento
X_train_scaled = X_train.copy()
X_train_scaled[continuous_features] = scaler.fit_transform(X_train[continuous_features])

# Usar el mismo escalador para transformar el conjunto de prueba
X_test_scaled = X_test.copy()
X_test_scaled[continuous_features] = scaler.transform(X_test[continuous_features])

# Verificar resultados del escalado
print("Características continuas escaladas:")
print(f"Media de entrenamiento después del escalado:\n{X_train_scaled[continuous_features].mean()}")
print(f"Desviación estándar de entrenamiento después del escalado:\n{X_train_scaled[continuous_features].std()}")







# What Data Should You Use for Training the Model:
# You should use:

# X_train_scaled: Scaled training features.
# y_train: Corresponding target variable for the training set.





# For Model Evaluation (Testing):
# You should use:

# X_test_scaled: Scaled testing features.
# y_test: Corresponding target variable for the testing set.



