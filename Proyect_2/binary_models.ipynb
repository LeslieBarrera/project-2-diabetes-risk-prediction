{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "División 0 vs no 0:\n",
      "Entrenamiento - Cantidad de 0: 149271, Cantidad de no 0: 30043\n",
      "Prueba - Cantidad de 0: 37318, Cantidad de no 0: 7511\n",
      "\n",
      "División 1 vs 2:\n",
      "Entrenamiento - Cantidad de 1: 3562, Cantidad de 2: 26481\n",
      "Prueba - Cantidad de 1: 891, Cantidad de 2: 6620\n"
     ]
    }
   ],
   "source": [
    "# Divide data into training and testing sets core    0 vs 1,2   and 1 vs 2\n",
    "\n",
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar el dataset\n",
    "input_file_path = \"cleaned_and_scaled_dataset.csv\"  # Cambiar si es necesario\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Primera transformación: 0 vs no 0\n",
    "df['y_binary_0_vs_non0'] = df['diabetes_012'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# Dividir en train y test para 0 vs no 0\n",
    "X = df.drop(columns=['diabetes_012', 'y_binary_0_vs_non0'])  # Eliminar columnas objetivo\n",
    "y_binary_0_vs_non0 = df['y_binary_0_vs_non0']\n",
    "\n",
    "X_train_0_vs_non0, X_test_0_vs_non0, y_train_0_vs_non0, y_test_0_vs_non0 = train_test_split(\n",
    "    X, y_binary_0_vs_non0, test_size=0.2, random_state=42, stratify=y_binary_0_vs_non0\n",
    ")\n",
    "\n",
    "# Contar valores en y para 0 vs no 0\n",
    "print(\"División 0 vs no 0:\")\n",
    "print(f\"Entrenamiento - Cantidad de 0: {sum(y_train_0_vs_non0 == 0)}, Cantidad de no 0: {sum(y_train_0_vs_non0 == 1)}\")\n",
    "print(f\"Prueba - Cantidad de 0: {sum(y_test_0_vs_non0 == 0)}, Cantidad de no 0: {sum(y_test_0_vs_non0 == 1)}\")\n",
    "\n",
    "# Segunda transformación: 1 vs 2 (datos donde diabetes_012 es 1 o 2)\n",
    "df_filtered_1_vs_2 = df[df['diabetes_012'].isin([1, 2])].copy()\n",
    "df_filtered_1_vs_2['y_binary_1_vs_2'] = df_filtered_1_vs_2['diabetes_012'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# Dividir en train y test para 1 vs 2\n",
    "X_filtered_1_vs_2 = df_filtered_1_vs_2.drop(columns=['diabetes_012', 'y_binary_1_vs_2'])\n",
    "y_binary_1_vs_2 = df_filtered_1_vs_2['y_binary_1_vs_2']\n",
    "\n",
    "X_train_1_vs_2, X_test_1_vs_2, y_train_1_vs_2, y_test_1_vs_2 = train_test_split(\n",
    "    X_filtered_1_vs_2, y_binary_1_vs_2, test_size=0.2, random_state=42, stratify=y_binary_1_vs_2\n",
    ")\n",
    "\n",
    "# Contar valores en y para 1 vs 2\n",
    "print(\"\\nDivisión 1 vs 2:\")\n",
    "print(f\"Entrenamiento - Cantidad de 1: {sum(y_train_1_vs_2 == 0)}, Cantidad de 2: {sum(y_train_1_vs_2 == 1)}\")\n",
    "print(f\"Prueba - Cantidad de 1: {sum(y_test_1_vs_2 == 0)}, Cantidad de 2: {sum(y_test_1_vs_2 == 1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train  XGBClassifier , RandomForestClassifier,  LogisticRegression  for 0 vs 1,2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proporción scale_pos_weight: 4.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [18:00:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de Confusión:\n",
      "[[27040 10278]\n",
      " [ 2072  5439]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.72      0.81     37318\n",
      "           1       0.35      0.72      0.47      7511\n",
      "\n",
      "    accuracy                           0.72     44829\n",
      "   macro avg       0.64      0.72      0.64     44829\n",
      "weighted avg       0.83      0.72      0.76     44829\n",
      "\n",
      "\n",
      "Precisión General del Modelo:\n",
      "0.72\n"
     ]
    }
   ],
   "source": [
    "#XGBClassifier for   0 vs 1,2    \n",
    "\n",
    "# Importar librerías necesarias\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Calcular scale_pos_weight\n",
    "n_zeros = sum(y_train_0_vs_non0 == 0)\n",
    "n_non_zeros = sum(y_train_0_vs_non0 == 1)\n",
    "scale_pos_weight = n_zeros / n_non_zeros\n",
    "\n",
    "print(f\"Proporción scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Instanciar el modelo XGBoost con scale_pos_weight\n",
    "model = XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,  # Manejo del desbalance\n",
    "    use_label_encoder=False,           # Desactivar codificación de etiquetas obsoleta\n",
    "    eval_metric='logloss',             # Métrica para clasificación binaria\n",
    "    random_state=42,\n",
    "    n_estimators=100,                  # Número de árboles\n",
    "    max_depth=10,                       # Profundidad máxima de los árboles\n",
    "    learning_rate=0.1                  # Tasa de aprendizaje\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train_0_vs_non0, y_train_0_vs_non0)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test_0_vs_non0)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_test_0_vs_non0, y_pred))\n",
    "\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test_0_vs_non0, y_pred))\n",
    "\n",
    "print(\"\\nPrecisión General del Modelo:\")\n",
    "print(f\"{accuracy_score(y_test_0_vs_non0, y_pred):.2f}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de Confusión:\n",
      "[[25806 11512]\n",
      " [ 1769  5742]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.69      0.80     37318\n",
      "           1       0.33      0.76      0.46      7511\n",
      "\n",
      "    accuracy                           0.70     44829\n",
      "   macro avg       0.63      0.73      0.63     44829\n",
      "weighted avg       0.83      0.70      0.74     44829\n",
      "\n",
      "\n",
      "Precisión General del Modelo:\n",
      "0.70\n"
     ]
    }
   ],
   "source": [
    "#RandomForestClassifier  for 0 vs 1,2\n",
    "\n",
    "# Importar librerías necesarias\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Instanciar el modelo Random Forest con class_weight='balanced_subsample'\n",
    "model = RandomForestClassifier(\n",
    "    class_weight='balanced_subsample',  # Ajuste automático de pesos por árbol\n",
    "    n_estimators=100,                  # Número de árboles en el bosque\n",
    "    max_depth=6,                    # Sin límite de profundidad (puedes ajustar según el dataset)\n",
    "    random_state=42,                   # Semilla para reproducibilidad\n",
    "    n_jobs=-1                          # Utiliza todos los núcleos disponibles para entrenar\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "model.fit(X_train_0_vs_non0, y_train_0_vs_non0)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test_0_vs_non0)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_test_0_vs_non0, y_pred))\n",
    "\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test_0_vs_non0, y_pred))\n",
    "\n",
    "print(\"\\nPrecisión General del Modelo:\")\n",
    "print(f\"{accuracy_score(y_test_0_vs_non0, y_pred):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de Confusión:\n",
      "[[26321 10997]\n",
      " [ 1817  5694]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.71      0.80     37318\n",
      "           1       0.34      0.76      0.47      7511\n",
      "\n",
      "    accuracy                           0.71     44829\n",
      "   macro avg       0.64      0.73      0.64     44829\n",
      "weighted avg       0.84      0.71      0.75     44829\n",
      "\n",
      "\n",
      "Precisión General del Modelo:\n",
      "0.71\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression  for 0 vs 1,2\n",
    "\n",
    "# Importar librerías necesarias\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Instanciar el modelo Logistic Regression con class_weight='balanced'\n",
    "model = LogisticRegression(\n",
    "    class_weight='balanced',  # Ajuste automático de pesos según el desbalance\n",
    "    solver='lbfgs',           # Algoritmo de optimización\n",
    "    max_iter=100,             # Número máximo de iteraciones\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "model.fit(X_train_0_vs_non0, y_train_0_vs_non0)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test_0_vs_non0)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_test_0_vs_non0, y_pred))\n",
    "\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test_0_vs_non0, y_pred))\n",
    "\n",
    "print(\"\\nPrecisión General del Modelo:\")\n",
    "print(f\"{accuracy_score(y_test_0_vs_non0, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de Confusión:\n",
      "[[   1  890]\n",
      " [   4 6616]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.00      0.00       891\n",
      "           1       0.88      1.00      0.94      6620\n",
      "\n",
      "    accuracy                           0.88      7511\n",
      "   macro avg       0.54      0.50      0.47      7511\n",
      "weighted avg       0.80      0.88      0.83      7511\n",
      "\n",
      "\n",
      "Precisión General del Modelo:\n",
      "0.88\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1 vs 2 \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Calcular scale_pos_weight\n",
    "n_class_1 = sum(y_train_1_vs_2 == 0)  # Clase 1 (ahora codificada como 0)\n",
    "n_class_2 = sum(y_train_1_vs_2 == 1)  # Clase 2 (ahora codificada como 1)\n",
    "scale_pos_weight = n_class_2 / n_class_1\n",
    "\n",
    "# Instanciar el modelo (sin use_label_encoder)\n",
    "model = XGBClassifier(\n",
    "    scale_pos_weight=15 ,\n",
    "    eval_metric='aucpr',  # Métrica para clasificación binaria\n",
    "    random_state=42,\n",
    "    n_estimators=300,       # Número de árboles\n",
    "    max_depth=15,            # Profundidad máxima de los árboles\n",
    "    learning_rate=0.005      # Tasa de aprendizaje\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train_1_vs_2, y_train_1_vs_2)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = model.predict(X_test_1_vs_2)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_test_1_vs_2, y_pred))\n",
    "\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test_1_vs_2, y_pred))\n",
    "\n",
    "print(\"\\nPrecisión General del Modelo:\")\n",
    "print(f\"{accuracy_score(y_test_1_vs_2, y_pred):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 26481, number of negative: 3562\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001288 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 154\n",
      "[LightGBM] [Info] Number of data points in the train set: 30043, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.881437 -> initscore=2.006105\n",
      "[LightGBM] [Info] Start training from score 2.006105\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[  44  847]\n",
      " [  81 6539]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.05      0.09       891\n",
      "           1       0.89      0.99      0.93      6620\n",
      "\n",
      "    accuracy                           0.88      7511\n",
      "   macro avg       0.62      0.52      0.51      7511\n",
      "weighted avg       0.82      0.88      0.83      7511\n",
      "\n",
      "\n",
      "Precisión General del Modelo:\n",
      "0.88\n"
     ]
    }
   ],
   "source": [
    "#LGBMClassifier  1 vs 2 \n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Instanciar el modelo LightGBM\n",
    "model = LGBMClassifier(\n",
    "    is_unbalance=True,    # Manejo automático del desbalance\n",
    "    random_state=42,      # Semilla para reproducibilidad\n",
    "    n_estimators=100,     # Número de árboles\n",
    "    max_depth=6,         # Profundidad máxima de los árboles\n",
    "    learning_rate=0.01    # Tasa de aprendizaje\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "model.fit(X_train_1_vs_2, y_train_1_vs_2)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = model.predict(X_test_1_vs_2)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_test_1_vs_2, y_pred))\n",
    "\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test_1_vs_2, y_pred))\n",
    "\n",
    "print(\"\\nPrecisión General del Modelo:\")\n",
    "print(f\"{accuracy_score(y_test_1_vs_2, y_pred):.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de Confusión:\n",
      "[[ 524  367]\n",
      " [2653 3967]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.59      0.26       891\n",
      "           1       0.92      0.60      0.72      6620\n",
      "\n",
      "    accuracy                           0.60      7511\n",
      "   macro avg       0.54      0.59      0.49      7511\n",
      "weighted avg       0.83      0.60      0.67      7511\n",
      "\n",
      "\n",
      "Precisión General del Modelo:\n",
      "0.60\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression 1 vs 2 \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Instanciar el modelo Logistic Regression\n",
    "model = LogisticRegression(\n",
    "    class_weight='balanced',  # Ajuste automático de pesos según el desbalance\n",
    "    solver='lbfgs',           # Algoritmo de optimización\n",
    "    max_iter=200,             # Número máximo de iteraciones\n",
    "    random_state=42           # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "model.fit(X_train_1_vs_2, y_train_1_vs_2)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = model.predict(X_test_1_vs_2)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_test_1_vs_2, y_pred))\n",
    "\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test_1_vs_2, y_pred))\n",
    "\n",
    "print(\"\\nPrecisión General del Modelo:\")\n",
    "print(f\"{accuracy_score(y_test_1_vs_2, y_pred):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución antes de SMOTE (clases 1 y 2):\n",
      "y_binary_1_vs_2\n",
      "1    26481\n",
      "0     3562\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución después de SMOTE (clases 1 y 2):\n",
      "y_binary_1_vs_2\n",
      "1    26481\n",
      "0    26481\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[ 404  487]\n",
      " [2286 4334]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.45      0.23       891\n",
      "           1       0.90      0.65      0.76      6620\n",
      "\n",
      "    accuracy                           0.63      7511\n",
      "   macro avg       0.52      0.55      0.49      7511\n",
      "weighted avg       0.81      0.63      0.69      7511\n",
      "\n",
      "\n",
      "Precisión General del Modelo:\n",
      "0.63\n"
     ]
    }
   ],
   "source": [
    "# xgboost  1 vs 2  with SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Verificar la distribución antes de SMOTE\n",
    "print(\"Distribución antes de SMOTE (clases 1 y 2):\")\n",
    "print(y_train_1_vs_2.value_counts())\n",
    "\n",
    "# Aplicar SMOTE para balancear las clases\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_1_vs_2, y_train_1_vs_2)\n",
    "\n",
    "# Verificar la distribución después de SMOTE\n",
    "print(\"\\nDistribución después de SMOTE (clases 1 y 2):\")\n",
    "print(pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "# Instanciar el modelo XGBoost\n",
    "model = XGBClassifier(\n",
    "    scale_pos_weight=1,        # No es necesario con SMOTE (clases ya balanceadas)\n",
    "    eval_metric='aucpr',       # Métrica para datasets desbalanceados\n",
    "    random_state=42,\n",
    "    n_estimators=150,          # Número de árboles\n",
    "    max_depth=6,              # Profundidad máxima de los árboles\n",
    "    learning_rate=0.00005       # Tasa de aprendizaje\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con los datos balanceados\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = model.predict(X_test_1_vs_2)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_test_1_vs_2, y_pred))\n",
    "\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test_1_vs_2, y_pred))\n",
    "print(\"\\nPrecisión General del Modelo:\")\n",
    "print(f\"{accuracy_score(y_test_1_vs_2, y_pred):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\n\\n\\n\\n models 1 vs 1,2 \\n\\nRandom Forest:\\nMacro Avg Recall: 0.73\\nMás consistente entre ambas clases.\\nLogistic Regression:\\nMacro Avg Recall: 0.73\\nSimilar a Random Forest, pero ligeramente menos robusto en la precisión general.\\nXGBoost:\\nMacro Avg Recall: 0.72\\nMenor recall en comparación con los otros modelos.\\n\\n\\n\\n\\n\\n\\n\\n\\nOrden de Modelos por Recall para 1 vs 2 \\nBasándonos en el recall balanceado entre ambas clases, el orden sería:\\n\\nLogistic Regression: is the best one \\nClase 0: Recall = 0.59\\nClase 1: Recall = 0.60\\n\\nXGBoost (Con SMOTE):\\nClase 0: Recall = 0.45\\nClase 1: Recall = 0.65\\n\\nLightGBM (Sin SMOTE):\\nClase 0: Recall = 0.05\\nClase 1: Recall = 0.99\\n\\nXGBoost (Sin SMOTE):\\nClase 0: Recall = 0.00\\nClase 1: Recall = 1.00'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "\n",
    "\n",
    "\n",
    "\n",
    " models 1 vs 1,2 \n",
    "\n",
    "Random Forest:\n",
    "Macro Avg Recall: 0.73\n",
    "Más consistente entre ambas clases.\n",
    "Logistic Regression:\n",
    "Macro Avg Recall: 0.73\n",
    "Similar a Random Forest, pero ligeramente menos robusto en la precisión general.\n",
    "XGBoost:\n",
    "Macro Avg Recall: 0.72\n",
    "Menor recall en comparación con los otros modelos.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Orden de Modelos por Recall para 1 vs 2 \n",
    "Basándonos en el recall balanceado entre ambas clases, el orden sería:\n",
    "\n",
    "Logistic Regression: is the best one \n",
    "Clase 0: Recall = 0.59\n",
    "Clase 1: Recall = 0.60\n",
    "\n",
    "XGBoost (Con SMOTE):\n",
    "Clase 0: Recall = 0.45\n",
    "Clase 1: Recall = 0.65\n",
    "\n",
    "LightGBM (Sin SMOTE):\n",
    "Clase 0: Recall = 0.05\n",
    "Clase 1: Recall = 0.99\n",
    "\n",
    "XGBoost (Sin SMOTE):\n",
    "Clase 0: Recall = 0.00\n",
    "Clase 1: Recall = 1.00\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 1 (0 vs no-0):\n",
      "[[25806 11512]\n",
      " [ 1769  5742]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.69      0.80     37318\n",
      "           1       0.33      0.76      0.46      7511\n",
      "\n",
      "    accuracy                           0.70     44829\n",
      "   macro avg       0.63      0.73      0.63     44829\n",
      "weighted avg       0.83      0.70      0.74     44829\n",
      "\n",
      "Modelo 2 (1 vs 2):\n",
      "[[ 524  367]\n",
      " [2650 3970]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.59      0.26       891\n",
      "           1       0.92      0.60      0.72      6620\n",
      "\n",
      "    accuracy                           0.60      7511\n",
      "   macro avg       0.54      0.59      0.49      7511\n",
      "weighted avg       0.83      0.60      0.67      7511\n",
      "\n",
      "Modelo combinado:\n",
      "[[25806  4676  6836]\n",
      " [ 1769  1550  4192]\n",
      " [    0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.69      0.80     37318\n",
      "           1       0.25      0.21      0.23      7511\n",
      "           2       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.61     44829\n",
      "   macro avg       0.39      0.30      0.34     44829\n",
      "weighted avg       0.82      0.61      0.70     44829\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# the join of best 2 model with no smote    \n",
    "# Modelo 1 (0 vs no-0): \n",
    "#Modelo 2 (1 vs 2): \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Paso 1: Cargar y preparar el dataset\n",
    "import pandas as pd\n",
    "input_file_path = \"cleaned_and_scaled_dataset.csv\"  # Cambia este archivo si es necesario\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# División 0 vs no-0\n",
    "X = df.drop(columns=['diabetes_012'])  # Características\n",
    "y = df['diabetes_012']  # Etiqueta objetivo\n",
    "\n",
    "y_binary = (y != 0).astype(int)  # Etiquetas binarias para 0 vs no-0\n",
    "X_train_0_vs_non0, X_test_0_vs_non0, y_train_0_vs_non0, y_test_0_vs_non0 = train_test_split(\n",
    "    X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "# División 1 vs 2\n",
    "df_filtered_1_vs_2 = df[df['diabetes_012'].isin([1, 2])].copy()\n",
    "X_filtered_1_vs_2 = df_filtered_1_vs_2.drop(columns=['diabetes_012'])\n",
    "y_filtered_1_vs_2 = (df_filtered_1_vs_2['diabetes_012'] - 1).astype(int)  # Etiquetas binarias para 1 vs 2\n",
    "\n",
    "X_train_1_vs_2, X_test_1_vs_2, y_train_1_vs_2, y_test_1_vs_2 = train_test_split(\n",
    "    X_filtered_1_vs_2, y_filtered_1_vs_2, test_size=0.2, random_state=42, stratify=y_filtered_1_vs_2\n",
    ")\n",
    "\n",
    "# Paso 2: Entrenar modelos\n",
    "# Modelo 1: Clasificador para 0 vs no-0\n",
    "model_rf = RandomForestClassifier(\n",
    "    class_weight='balanced_subsample',\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model_rf.fit(X_train_0_vs_non0, y_train_0_vs_non0)\n",
    "\n",
    "# Modelo 2: Clasificador para 1 vs 2\n",
    "model_lr = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    solver='lbfgs',\n",
    "    max_iter=200,\n",
    "    random_state=42\n",
    ")\n",
    "model_lr.fit(X_train_1_vs_2, y_train_1_vs_2)\n",
    "\n",
    "# Paso 3: Predicciones para cada modelo\n",
    "# Modelo 1: Evaluar 0 vs no-0\n",
    "y_pred_0_vs_non0 = model_rf.predict(X_test_0_vs_non0)\n",
    "print(\"Modelo 1 (0 vs no-0):\")\n",
    "print(confusion_matrix(y_test_0_vs_non0, y_pred_0_vs_non0))\n",
    "print(classification_report(y_test_0_vs_non0, y_pred_0_vs_non0))\n",
    "\n",
    "# Modelo 2: Evaluar 1 vs 2\n",
    "y_pred_1_vs_2 = model_lr.predict(X_test_1_vs_2)\n",
    "print(\"Modelo 2 (1 vs 2):\")\n",
    "print(confusion_matrix(y_test_1_vs_2, y_pred_1_vs_2))\n",
    "print(classification_report(y_test_1_vs_2, y_pred_1_vs_2))\n",
    "\n",
    "# Paso 4: Modelo combinado\n",
    "def combined_model_predict(X, model_rf, model_lr):\n",
    "    # Predicciones del modelo 0 vs no-0\n",
    "    pred_m1 = model_rf.predict(X)\n",
    "    \n",
    "    # Filtrar índices para 1 vs 2\n",
    "    non_zero_indices = np.where(pred_m1 == 1)[0]\n",
    "    final_pred = np.copy(pred_m1)  # Inicializa con las predicciones de 0 vs no-0\n",
    "    \n",
    "    if len(non_zero_indices) > 0:\n",
    "        X_non_zero = X.iloc[non_zero_indices]  # Filtrar características no-0\n",
    "        pred_m2 = model_lr.predict(X_non_zero)  # Predicciones para 1 vs 2\n",
    "        final_pred[non_zero_indices] = pred_m2 + 1  # Ajustar etiquetas a 1 y 2\n",
    "    \n",
    "    return final_pred\n",
    "\n",
    "# Realizar predicciones combinadas\n",
    "y_pred_combined = combined_model_predict(X_test_0_vs_non0, model_rf, model_lr)\n",
    "\n",
    "# Paso 5: Evaluar modelo combinado\n",
    "print(\"Modelo combinado:\")\n",
    "print(confusion_matrix(y_test_0_vs_non0, y_pred_combined))\n",
    "print(classification_report(y_test_0_vs_non0, y_pred_combined))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
